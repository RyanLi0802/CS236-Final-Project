{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwDxHsbncEGqKdB5crpNqq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"nzG1vXTsla6g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702260722738,"user_tz":480,"elapsed":16534,"user":{"displayName":"Churan He","userId":"11797137588437152658"}},"outputId":"138a1715-ef18-4101-c686-eec990895b1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["! pip install pytorch torchvision cudatoolkit\n","! pip install ftfy regex tqdm\n","! pip install git+https://github.com/openai/CLIP.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3z076cMlze7","executionInfo":{"status":"ok","timestamp":1702260801367,"user_tz":480,"elapsed":21338,"user":{"displayName":"Churan He","userId":"11797137588437152658"}},"outputId":"670132c3-3cb8-4a50-81e2-a47ad909d410"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch\n","  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement cudatoolkit (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for cudatoolkit\u001b[0m\u001b[31m\n","\u001b[0mCollecting ftfy\n","  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.12)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.3\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-2l828xiu\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-2l828xiu\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=c1ea36106fc920c4ce96985286682f86a2fa82b4288c4cd3686ddd28b00dc350\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-iw2r9jr2/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n"]}]},{"cell_type":"code","source":["# test code\n","import torch\n","import clip\n","from PIL import Image\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","image = preprocess(Image.open(\"/content/drive/Shareddrives/CS236/final_project/frames/tiger/frame_0.png\")).unsqueeze(0).to(device)\n","text = clip.tokenize([\"a tiger\", \"a dog\", \"a cat\"]).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image)\n","    text_features = model.encode_text(text)\n","\n","    logits_per_image, logits_per_text = model(image, text)\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","print(\"Label probs:\", probs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"waejXmXcmVZX","executionInfo":{"status":"ok","timestamp":1702260942031,"user_tz":480,"elapsed":6590,"user":{"displayName":"Churan He","userId":"11797137588437152658"}},"outputId":"a3d88004-b0a8-4e39-c8d1-ee1c64f21078"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Label probs: [[9.9609691e-01 2.4044437e-04 3.6626642e-03]]\n"]}]},{"cell_type":"markdown","source":["Text-Video Consistency (CLIP-Score)"],"metadata":{"id":"YZBboxBSnt6h"}},{"cell_type":"code","source":["import os\n","import torch\n","import clip\n","from PIL import Image\n","cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n","\n","#frames_dir = \"/content/drive/Shareddrives/CS236/final_project/frames/golden_retriever\"\n","frames_dir = \"/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_walking_30000\"\n","frames = os.listdir(frames_dir)\n","frames.sort()\n","frames = frames[0:8]\n","\n","#embedding = \"dog plays in the water\"\n","embedding = \"tiger walking in grass\"\n","\n","result = []\n","\n","for frame in frames:\n","  print(os.path.join(frames_dir, frame))\n","  image = preprocess(Image.open(os.path.join(frames_dir, frame))).unsqueeze(0).to(device)\n","  text = clip.tokenize(embedding).to(device)\n","  with torch.no_grad():\n","      image_features = model.encode_image(image)\n","      text_features = model.encode_text(text)\n","      cos_similarity = cos(image_features, text_features)\n","      print(cos_similarity)\n","      result.append(cos_similarity)\n","print(\"result: \"+str((sum(result)/len(result)) * 100))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5OskIOcn05p","executionInfo":{"status":"ok","timestamp":1702266616708,"user_tz":480,"elapsed":4451,"user":{"displayName":"Churan He","userId":"11797137588437152658"}},"outputId":"281babf8-1857-4c7b-f119-3281e2a9cff5"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_walking_30000/frame_0.png\n","tensor([0.3113])\n","/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_walking_30000/frame_1.png\n","tensor([0.3002])\n","/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_walking_30000/frame_2.png\n","tensor([0.3077])\n","/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_walking_30000/frame_3.png\n","tensor([0.3011])\n","/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_walking_30000/frame_4.png\n","tensor([0.3064])\n","/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_walking_30000/frame_5.png\n","tensor([0.3095])\n","/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_walking_30000/frame_6.png\n","tensor([0.2956])\n","/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_walking_30000/frame_7.png\n","tensor([0.2915])\n","result: tensor([30.2914])\n"]}]},{"cell_type":"markdown","source":["Text-Video Consistency (CLIP-Temp)"],"metadata":{"id":"Zxgxan4Lu7HA"}},{"cell_type":"code","source":["import os\n","import torch\n","import clip\n","from PIL import Image\n","cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n","\n","frames_dir = \"/content/drive/Shareddrives/CS236/final_project/frames/baseline_no_inv\"\n","frames = os.listdir(frames_dir)\n","frames.sort()\n","frames = frames[0:8]\n","\n","result = []\n","\n","for i in range(len(frames) - 1):\n","  image_1 = preprocess(Image.open(os.path.join(frames_dir, frames[i]))).unsqueeze(0).to(device)\n","  image_2 = preprocess(Image.open(os.path.join(frames_dir, frames[i+1]))).unsqueeze(0).to(device)\n","  with torch.no_grad():\n","      image_1_features = model.encode_image(image_1)\n","      image_2_features = model.encode_image(image_2)\n","      cos_similarity = cos(image_1_features, image_2_features)\n","      print(cos_similarity)\n","      result.append(cos_similarity)\n","print(\"result: \"+str((sum(result)/len(result)) * 100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6Xyf2tTu_bS","executionInfo":{"status":"ok","timestamp":1702266849457,"user_tz":480,"elapsed":3073,"user":{"displayName":"Churan He","userId":"11797137588437152658"}},"outputId":"e2d45f2a-b4aa-4ae6-f39a-11e2a80959d4"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.9134])\n","tensor([0.9701])\n","tensor([0.9632])\n","tensor([0.9586])\n","tensor([0.9617])\n","tensor([0.9575])\n","tensor([0.9616])\n","result: tensor([95.5152])\n"]}]},{"cell_type":"markdown","source":["Warping Error"],"metadata":{"id":"IEcKjYXFwFUq"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","\n","frames_dir = \"/content/drive/Shareddrives/CS236/final_project/main/results_fewshot/frames/tiger_snow_30000\"\n","frames = os.listdir(frames_dir)\n","frames.sort()\n","frames = frames[0:8]\n","\n","result = []\n","\n","for i in range(len(frames) - 1):\n","\n","  # Load images (Assuming they are already read into image_1 and image_2)\n","  image_1 = cv2.imread(os.path.join(frames_dir, frames[i]))\n","  image_2 = cv2.imread(os.path.join(frames_dir, frames[i+1]))\n","\n","  # Initialize feature detector (ORB, SIFT, SURF can also be used)\n","  orb = cv2.ORB_create()\n","\n","  # Find keypoints and descriptors\n","  kp1, des1 = orb.detectAndCompute(image_1, None)\n","  kp2, des2 = orb.detectAndCompute(image_2, None)\n","\n","  # Create a matcher and find matches (FLANN based or BFMatcher can also be used)\n","  matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","  matches = matcher.match(des1, des2)\n","\n","  # Sort matches by distance (best matches first)\n","  matches = sorted(matches, key=lambda x: x.distance)\n","\n","  # Extract location of good matches\n","  points1 = np.zeros((len(matches), 2), dtype=np.float32)\n","  points2 = np.zeros_like(points1)\n","\n","  for i, match in enumerate(matches):\n","      points1[i, :] = kp1[match.queryIdx].pt\n","      points2[i, :] = kp2[match.trainIdx].pt\n","\n","  # Find homography (transformation matrix)\n","  H, _ = cv2.findHomography(points1, points2, cv2.RANSAC)\n","\n","  warped_image_1 = cv2.warpPerspective(image_1, H, (image_2.shape[1], image_2.shape[0]))\n","\n","  error = np.sum((warped_image_1.astype(\"float\") - image_2.astype(\"float\")) ** 2)\n","  error /= float(warped_image_1.shape[0] * warped_image_1.shape[1])\n","\n","  result.append(error)\n","\n","print(str((sum(result)/len(result))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAxmnhPXwI9k","executionInfo":{"status":"ok","timestamp":1702267219893,"user_tz":480,"elapsed":4354,"user":{"displayName":"Churan He","userId":"11797137588437152658"}},"outputId":"6c13f04e-4835-4021-9898-209842daa432"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["7756.696088518415\n"]}]}]}